import { Request, Response, NextFunction } from 'express';
import { prisma } from '../server.js';
import { ApiError } from '../middlewares/error.middleware.js';

import { AIService } from '../services/ai.service.js';

// AI Service for RAG (Retrieval Augmented Generation)
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'gemini-3-flash-preview:cloud';

// Serialize BigInt for JSON
const serializeData = (data: any) => {
  return JSON.parse(JSON.stringify(data, (key, value) =>
    typeof value === 'bigint' ? value.toString() : value
  ));
};

export const chat = async (
  req: Request,
  res: Response,
  next: NextFunction
) => {
  try {
    const { message, history } = req.body;

    if (!message || typeof message !== 'string' || message.trim().length === 0) {
      throw new ApiError(400, 'Message is required');
    }

    // Generate AI response using Enhanced AI Service
    let aiResponse: { message: string; products: any[]; orders: any[] } = { message: '', products: [], orders: [] };
    
    try {
      const formattedHistory = (history || []).map((h: any) => ({
        role: h.role,
        content: h.content
      }));

      const response = await AIService.generateCustomerResponse(formattedHistory, message, (req as any).user);
      
      aiResponse = {
          message: response.message,
          products: response.products || [],
          orders: response.orders || []
      };

    } catch (error) {
      console.error('AI Service Error:', error);
      aiResponse.message = 'Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau.';
    }

    // Detect quick reply suggestions based on context
    const quickReplies = generateQuickReplies(message, aiResponse.message, !!(req as any).user);

    // Return response with matched products and quick replies
    res.json({
      success: true,
      data: {
        message: aiResponse.message,
        products: serializeData(aiResponse.products || []),
        orders: serializeData(aiResponse.orders || []),
        quickReplies
      }
    });

  } catch (error) {
    next(error);
  }
};

// Generate contextual quick reply suggestions
function generateQuickReplies(userMessage: string, aiResponse: string, isLoggedIn: boolean): string[] {
  const replies: string[] = [];
  const msg = userMessage.toLowerCase();
  const resp = aiResponse.toLowerCase();

  // After product search, suggest related actions
  if (resp.includes('sáº£n pháº©m') || resp.includes('tÃ¬m tháº¥y')) {
    replies.push('Xem thÃªm sáº£n pháº©m tÆ°Æ¡ng tá»±');
    replies.push('CÃ³ mÃ£ giáº£m giÃ¡ khÃ´ng?');
    if (isLoggedIn) replies.push('ThÃªm vÃ o giá» hÃ ng');
  }
  // After order inquiry
  else if (resp.includes('Ä‘Æ¡n hÃ ng') || resp.includes('order')) {
    replies.push('Theo dÃµi Ä‘Æ¡n hÃ ng');
    replies.push('ChÃ­nh sÃ¡ch Ä‘á»•i tráº£');
  }
  // After greeting
  else if (msg.includes('chÃ o') || msg.includes('hello') || msg.includes('hi')) {
    replies.push('ðŸ†• HÃ ng má»›i vá»');
    replies.push('ðŸ”¥ Sáº£n pháº©m hot');
    replies.push('ðŸŽ« MÃ£ giáº£m giÃ¡');
    if (isLoggedIn) replies.push('ðŸ“¦ ÄÆ¡n hÃ ng cá»§a tÃ´i');
  }
  // After outfit/fashion advice 
  else if (msg.includes('máº·c gÃ¬') || msg.includes('outfit') || msg.includes('phá»‘i')) {
    replies.push('Xem sáº£n pháº©m gá»£i Ã½');
    replies.push('TÆ° váº¥n thÃªm phong cÃ¡ch khÃ¡c');
  }
  // After coupon inquiry
  else if (resp.includes('giáº£m giÃ¡') || resp.includes('coupon') || resp.includes('khuyáº¿n mÃ£i')) {
    replies.push('Xem sáº£n pháº©m bÃ¡n cháº¡y');
    replies.push('TÃ¬m theo ngÃ¢n sÃ¡ch');
  }
  // Default suggestions
  else {
    replies.push('TÃ¬m sáº£n pháº©m');
    if (isLoggedIn) replies.push('ÄÆ¡n hÃ ng cá»§a tÃ´i');
    replies.push('TÆ° váº¥n thá»i trang');
  }

  return replies.slice(0, 4);
}

// Health check for AI service
export const checkAIHealth = async (
  req: Request,
  res: Response,
  next: NextFunction
) => {
  try {
    const response = await fetch(`${OLLAMA_URL}/api/tags`, {
      method: 'GET'
    });

    if (response.ok) {
      const data = await response.json() as { models?: Array<{ name: string }> };
      res.json({
        success: true,
        data: {
          status: 'available',
          models: data.models?.map((m) => m.name) || []
        }
      });
    } else {
      res.json({
        success: true,
        data: {
          status: 'unavailable',
          message: 'Ollama service is not responding'
        }
      });
    }
  } catch (error) {
    res.json({
      success: true,
      data: {
        status: 'unavailable',
        message: 'Cannot connect to Ollama service'
      }
    });
  }
  
};
